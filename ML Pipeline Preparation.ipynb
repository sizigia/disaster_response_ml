{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "### 1. Import libraries and load data from database\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/faustina/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/faustina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "nltk.download(['averaged_perceptron_tagger', 'wordnet'])\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from helpers impr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///web_app/data/DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterResponseData', engine)\n",
    "X = df['message']\n",
    "Y = df[df.columns[-36:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Transforms a text to clean tokens, where every token is a word converted to lower case,\n",
    "    passed to a part-of-speech tagger and lemmatized accordingly.\n",
    "    Words recognized as stopwords are ommitted.\n",
    "    \n",
    "    Input:\n",
    "        text (str)\n",
    "        \n",
    "    Output:\n",
    "        clean_tokens (list): list of clean tokens (words converted to lower case and lemmatized)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    clean_tokens = []\n",
    "    \n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag[0] in ['A', 'R', 'N', 'V']:\n",
    "            tag = tag[0].lower()\n",
    "            clean_token = lemmatizer.lemmatize(word, pos=tag)\n",
    "        else:\n",
    "            clean_token = word\n",
    "            \n",
    "        if clean_token not in stopwords.words('english'):\n",
    "            clean_tokens.append(clean_token)\n",
    "        \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline takes in the `message` column as input and outputs classification results on the other 36 categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(warm_start=True))),\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 4.4min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.1s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=23.9min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=True),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test model\n",
    "Report the precision, recall, f1 score for each output category of the dataset, and overall accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "\tPrecision: 0.81%\n",
      "\tRecall: 0.82%\n",
      "\tF1 Score: 0.81%\n",
      "\n",
      "request\n",
      "\tPrecision: 0.90%\n",
      "\tRecall: 0.90%\n",
      "\tF1 Score: 0.89%\n",
      "\n",
      "offer\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "aid_related\n",
      "\tPrecision: 0.77%\n",
      "\tRecall: 0.78%\n",
      "\tF1 Score: 0.77%\n",
      "\n",
      "medical_help\n",
      "\tPrecision: 0.91%\n",
      "\tRecall: 0.93%\n",
      "\tF1 Score: 0.90%\n",
      "\n",
      "medical_products\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faustina/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_and_rescue\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.96%\n",
      "\n",
      "security\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "military\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.96%\n",
      "\n",
      "child_alone\n",
      "\tPrecision: 1.00%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 1.00%\n",
      "\n",
      "water\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "food\n",
      "\tPrecision: 0.94%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "shelter\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "clothing\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "money\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "missing_people\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "refugees\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "death\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "other_aid\n",
      "\tPrecision: 0.83%\n",
      "\tRecall: 0.87%\n",
      "\tF1 Score: 0.82%\n",
      "\n",
      "infrastructure_related\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.90%\n",
      "\n",
      "transport\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "buildings\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "electricity\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "tools\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "hospitals\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "shops\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "aid_centers\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "other_infrastructure\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "weather_related\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.88%\n",
      "\tF1 Score: 0.88%\n",
      "\n",
      "floods\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "storm\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "fire\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "earthquake\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "cold\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "other_weather\n",
      "\tPrecision: 0.92%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "direct_report\n",
      "\tPrecision: 0.85%\n",
      "\tRecall: 0.86%\n",
      "\tF1 Score: 0.84%\n",
      "\n",
      "Accuracy Score: 0.95%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.948767504153528"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = pipeline.predict(X_test)\n",
    "\n",
    "report_classification(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x7f05ea935c20>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          max_samples=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=100,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=True),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': True,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x7f05ea935c20>,\n",
       "                 vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        max_samples=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_jobs=None,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=None,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=True),\n",
       "                       n_jobs=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': True,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=True),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test model\n",
    "Show the precision, recall and overall accuracy of the tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = cv.predict(X_test)\n",
    "\n",
    "report_classification(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Improve model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('web_app/models/classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(cv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile web_app/models/train_classifier.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
