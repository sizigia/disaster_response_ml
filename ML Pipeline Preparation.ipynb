{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "### 1. Import libraries and load data from database\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/faustina/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/faustina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "nltk.download(['averaged_perceptron_tagger', 'wordnet'])\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///web_app/data/DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterResponseData', engine)\n",
    "X = df['message']\n",
    "Y = df[df.columns[-36:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Transforms a text to clean tokens, where every token is a word converted to lower case,\n",
    "    passed to a part-of-speech tagger and lemmatized accordingly.\n",
    "    Words recognized as stopwords are ommitted.\n",
    "    \n",
    "    Input:\n",
    "        text (str)\n",
    "        \n",
    "    Output:\n",
    "        clean_tokens (list): list of clean tokens (words converted to lower case and lemmatized)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    clean_tokens = []\n",
    "    \n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag[0] in ['A', 'R', 'N', 'V']:\n",
    "            tag = tag[0].lower()\n",
    "            clean_token = lemmatizer.lemmatize(word, pos=tag)\n",
    "        else:\n",
    "            clean_token = word\n",
    "            \n",
    "        if clean_token not in stopwords.words('english'):\n",
    "            clean_tokens.append(clean_token)\n",
    "        \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline takes in the `message` column as input and outputs classification results on the other 36 categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier())),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_processing(model=None, X_subset=None, Y_subset=None, n_batches = 101):\n",
    "    \"\"\"\n",
    "    Takes in X and Y subsets and fits a model with it. Returns the model after processing all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert X_subset.shape[0] == Y_subset.shape[0]\n",
    "    \n",
    "    batch_size = X_subset.shape[0] // n_batches\n",
    "\n",
    "    it = itertools.count(step=batch_size)\n",
    "\n",
    "    for _ in range(n_batches):\n",
    "        start = next(it)\n",
    "        end = start + batch_size\n",
    "        \n",
    "        model.fit(X_subset[start : end], Y_subset[start : end])\n",
    "        \n",
    "        print(f\"{end} trained examples. {round(end / X_subset.shape[0] * 100, 2)}%\")\n",
    "\n",
    "        if (end + batch_size) > X_subset.shape[0]:\n",
    "            assert (end + len(X_subset[end:])) == X_subset.shape[0]\n",
    "            model.fit(X_subset[end:], Y_subset[end:])\n",
    "            print(f\"{end + len(X_subset[end:])} trained examples. {round((end + len(X_subset[end:])) / X_subset.shape[0] * 100, 2)}%\")\n",
    "    \n",
    "    return model            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 trained examples. 0.99%\n",
      "388 trained examples. 1.97%\n",
      "582 trained examples. 2.96%\n",
      "776 trained examples. 3.95%\n",
      "970 trained examples. 4.93%\n",
      "1164 trained examples. 5.92%\n",
      "1358 trained examples. 6.91%\n",
      "1552 trained examples. 7.89%\n",
      "1746 trained examples. 8.88%\n",
      "1940 trained examples. 9.87%\n",
      "2134 trained examples. 10.85%\n",
      "2328 trained examples. 11.84%\n",
      "2522 trained examples. 12.83%\n",
      "2716 trained examples. 13.81%\n",
      "2910 trained examples. 14.8%\n",
      "3104 trained examples. 15.79%\n",
      "3298 trained examples. 16.77%\n",
      "3492 trained examples. 17.76%\n",
      "3686 trained examples. 18.75%\n",
      "3880 trained examples. 19.73%\n",
      "4074 trained examples. 20.72%\n",
      "4268 trained examples. 21.71%\n",
      "4462 trained examples. 22.69%\n",
      "4656 trained examples. 23.68%\n",
      "4850 trained examples. 24.67%\n",
      "5044 trained examples. 25.65%\n",
      "5238 trained examples. 26.64%\n",
      "5432 trained examples. 27.63%\n",
      "5626 trained examples. 28.62%\n",
      "5820 trained examples. 29.6%\n",
      "6014 trained examples. 30.59%\n",
      "6208 trained examples. 31.58%\n",
      "6402 trained examples. 32.56%\n",
      "6596 trained examples. 33.55%\n",
      "6790 trained examples. 34.54%\n",
      "6984 trained examples. 35.52%\n",
      "7178 trained examples. 36.51%\n",
      "7372 trained examples. 37.5%\n",
      "7566 trained examples. 38.48%\n",
      "7760 trained examples. 39.47%\n",
      "7954 trained examples. 40.46%\n",
      "8148 trained examples. 41.44%\n",
      "8342 trained examples. 42.43%\n",
      "8536 trained examples. 43.42%\n",
      "8730 trained examples. 44.4%\n",
      "8924 trained examples. 45.39%\n",
      "9118 trained examples. 46.38%\n",
      "9312 trained examples. 47.36%\n",
      "9506 trained examples. 48.35%\n",
      "9700 trained examples. 49.34%\n",
      "9894 trained examples. 50.32%\n",
      "10088 trained examples. 51.31%\n",
      "10282 trained examples. 52.3%\n",
      "10476 trained examples. 53.28%\n",
      "10670 trained examples. 54.27%\n",
      "10864 trained examples. 55.26%\n",
      "11058 trained examples. 56.24%\n",
      "11252 trained examples. 57.23%\n",
      "11446 trained examples. 58.22%\n",
      "11640 trained examples. 59.2%\n",
      "11834 trained examples. 60.19%\n",
      "12028 trained examples. 61.18%\n",
      "12222 trained examples. 62.16%\n",
      "12416 trained examples. 63.15%\n",
      "12610 trained examples. 64.14%\n",
      "12804 trained examples. 65.12%\n",
      "12998 trained examples. 66.11%\n",
      "13192 trained examples. 67.1%\n",
      "13386 trained examples. 68.08%\n",
      "13580 trained examples. 69.07%\n",
      "13774 trained examples. 70.06%\n",
      "13968 trained examples. 71.04%\n",
      "14162 trained examples. 72.03%\n",
      "14356 trained examples. 73.02%\n",
      "14550 trained examples. 74.0%\n",
      "14744 trained examples. 74.99%\n",
      "14938 trained examples. 75.98%\n",
      "15132 trained examples. 76.96%\n",
      "15326 trained examples. 77.95%\n",
      "15520 trained examples. 78.94%\n",
      "15714 trained examples. 79.92%\n",
      "15908 trained examples. 80.91%\n",
      "16102 trained examples. 81.9%\n",
      "16296 trained examples. 82.88%\n",
      "16490 trained examples. 83.87%\n",
      "16684 trained examples. 84.86%\n",
      "16878 trained examples. 85.85%\n",
      "17072 trained examples. 86.83%\n",
      "17266 trained examples. 87.82%\n",
      "17460 trained examples. 88.81%\n",
      "17654 trained examples. 89.79%\n",
      "17848 trained examples. 90.78%\n",
      "18042 trained examples. 91.77%\n",
      "18236 trained examples. 92.75%\n",
      "18430 trained examples. 93.74%\n",
      "18624 trained examples. 94.73%\n",
      "18818 trained examples. 95.71%\n",
      "19012 trained examples. 96.7%\n",
      "19206 trained examples. 97.69%\n",
      "19400 trained examples. 98.67%\n",
      "19594 trained examples. 99.66%\n",
      "19661 trained examples. 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_processing(pipeline, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test model\n",
    "Report the precision, recall, f1 score for each output category of the dataset, and overall accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_classification(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Takes in Y_test and Y_pred and returns precision, recall and F1 score \n",
    "    for every feature in the dataset, and the overall accuracy of the model.\n",
    "    \n",
    "    Input:\n",
    "        Y_test (pandas.core.series.Series): a subset of Y with the purpose of testing the model\n",
    "        Y_pred (pandas.core.series.Series): predictions made with X_test by the model\n",
    "        \n",
    "    Output:\n",
    "        Prints out the following format\n",
    "            feature_name\n",
    "                Precision: __%\n",
    "                Recall: __%\n",
    "                F1 Score: __%\n",
    "                \n",
    "                ...\n",
    "                \n",
    "                Accuracy Score: __%\n",
    "                \n",
    "        And also returns the full value of accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    for idx, col in enumerate(y_test):\n",
    "        set_y_pair = (y_test[col], y_pred[:, idx])\n",
    "        avg='weighted'\n",
    "        rep_col = \"{}\\n\\tPrecision: {:.2f}%\\n\\tRecall: {:.2f}%\\n\\tF1 Score: {:.2f}%\\n\".format(col,\n",
    "                                                                                 precision_score(*set_y_pair, average=avg), \n",
    "                                                                                 recall_score(*set_y_pair, average=avg), \n",
    "                                                                                 f1_score(*set_y_pair, average=avg))\n",
    "        print(rep_col)\n",
    "        \n",
    "    print('Accuracy Score: {:.2f}%'.format(np.mean(y_test.values == y_pred)))\n",
    "\n",
    "    return np.mean(y_test.values == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "\tPrecision: 0.82%\n",
      "\tRecall: 0.77%\n",
      "\tF1 Score: 0.67%\n",
      "\n",
      "request\n",
      "\tPrecision: 0.84%\n",
      "\tRecall: 0.84%\n",
      "\tF1 Score: 0.78%\n",
      "\n",
      "offer\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "aid_related\n",
      "\tPrecision: 0.68%\n",
      "\tRecall: 0.62%\n",
      "\tF1 Score: 0.53%\n",
      "\n",
      "medical_help\n",
      "\tPrecision: 0.85%\n",
      "\tRecall: 0.92%\n",
      "\tF1 Score: 0.88%\n",
      "\n",
      "medical_products\n",
      "\tPrecision: 0.90%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "search_and_rescue\n",
      "\tPrecision: 0.94%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.96%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faustina/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "security\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "military\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "child_alone\n",
      "\tPrecision: 1.00%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 1.00%\n",
      "\n",
      "water\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.90%\n",
      "\n",
      "food\n",
      "\tPrecision: 0.78%\n",
      "\tRecall: 0.88%\n",
      "\tF1 Score: 0.83%\n",
      "\n",
      "shelter\n",
      "\tPrecision: 0.83%\n",
      "\tRecall: 0.91%\n",
      "\tF1 Score: 0.87%\n",
      "\n",
      "clothing\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "money\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "missing_people\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "refugees\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "death\n",
      "\tPrecision: 0.91%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "other_aid\n",
      "\tPrecision: 0.79%\n",
      "\tRecall: 0.87%\n",
      "\tF1 Score: 0.81%\n",
      "\n",
      "infrastructure_related\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.91%\n",
      "\n",
      "transport\n",
      "\tPrecision: 0.91%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "buildings\n",
      "\tPrecision: 0.90%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "electricity\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "tools\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "hospitals\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "shops\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "aid_centers\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "other_infrastructure\n",
      "\tPrecision: 0.92%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "weather_related\n",
      "\tPrecision: 0.78%\n",
      "\tRecall: 0.73%\n",
      "\tF1 Score: 0.62%\n",
      "\n",
      "floods\n",
      "\tPrecision: 0.84%\n",
      "\tRecall: 0.92%\n",
      "\tF1 Score: 0.87%\n",
      "\n",
      "storm\n",
      "\tPrecision: 0.83%\n",
      "\tRecall: 0.91%\n",
      "\tF1 Score: 0.87%\n",
      "\n",
      "fire\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "earthquake\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.90%\n",
      "\tF1 Score: 0.86%\n",
      "\n",
      "cold\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "other_weather\n",
      "\tPrecision: 0.90%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "direct_report\n",
      "\tPrecision: 0.74%\n",
      "\tRecall: 0.80%\n",
      "\tF1 Score: 0.71%\n",
      "\n",
      "Accuracy Score: 0.93%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9278303326213"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = pipeline.predict(X_test)\n",
    "\n",
    "report_classification(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x7fa64324e680>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          max_samples=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=100,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=False),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x7fa64324e680>,\n",
       "                 vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        max_samples=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_jobs=None,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=None,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=False),\n",
       "                       n_jobs=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_processing(cv, X_train, Y_train, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test model\n",
    "Show the precision, recall and overall accuracy of the tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "report_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Improve model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile web_app/models/train_classifier.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
