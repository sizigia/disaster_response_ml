{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "### 1. Import libraries and load data from database\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/faustina/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/faustina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "nltk.download(['averaged_perceptron_tagger', 'wordnet'])\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///web_app/data/DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterResponseData', engine)\n",
    "X = df['message']\n",
    "Y = df[df.columns[-36:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Transforms a text to clean tokens, where every token is a word converted to lower case,\n",
    "    passed to a part-of-speech tagger and lemmatized accordingly.\n",
    "    Words recognized as stopwords are ommitted.\n",
    "    \n",
    "    Input:\n",
    "        text (str)\n",
    "        \n",
    "    Output:\n",
    "        clean_tokens (list): list of clean tokens (words converted to lower case and lemmatized)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    clean_tokens = []\n",
    "    \n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag[0] in ['A', 'R', 'N', 'V']:\n",
    "            tag = tag[0].lower()\n",
    "            clean_token = lemmatizer.lemmatize(word, pos=tag)\n",
    "        else:\n",
    "            clean_token = word\n",
    "            \n",
    "        if clean_token not in stopwords.words('english'):\n",
    "            clean_tokens.append(clean_token)\n",
    "        \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline takes in the `message` column as input and outputs classification results on the other 36 categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier())),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Trained 100 examples. 0.51% completed.\n",
      "\n",
      " Trained 200 examples. 1.02% completed.\n",
      "\n",
      " Trained 300 examples. 1.53% completed.\n",
      "\n",
      " Trained 400 examples. 2.04% completed.\n",
      "\n",
      " Trained 500 examples. 2.54% completed.\n",
      "\n",
      " Trained 600 examples. 3.05% completed.\n",
      "\n",
      " Trained 700 examples. 3.56% completed.\n",
      "\n",
      " Trained 800 examples. 4.07% completed.\n",
      "\n",
      " Trained 900 examples. 4.58% completed.\n",
      "\n",
      " Trained 1000 examples. 5.09% completed.\n",
      "\n",
      " Trained 1100 examples. 5.6% completed.\n",
      "\n",
      " Trained 1200 examples. 6.11% completed.\n",
      "\n",
      " Trained 1300 examples. 6.61% completed.\n",
      "\n",
      " Trained 1400 examples. 7.12% completed.\n",
      "\n",
      " Trained 1500 examples. 7.63% completed.\n",
      "\n",
      " Trained 1600 examples. 8.14% completed.\n",
      "\n",
      " Trained 1700 examples. 8.65% completed.\n",
      "\n",
      " Trained 1800 examples. 9.16% completed.\n",
      "\n",
      " Trained 1900 examples. 9.67% completed.\n",
      "\n",
      " Trained 2000 examples. 10.18% completed.\n",
      "\n",
      " Trained 2100 examples. 10.68% completed.\n",
      "\n",
      " Trained 2200 examples. 11.19% completed.\n",
      "\n",
      " Trained 2300 examples. 11.7% completed.\n",
      "\n",
      " Trained 2400 examples. 12.21% completed.\n",
      "\n",
      " Trained 2500 examples. 12.72% completed.\n",
      "\n",
      " Trained 2600 examples. 13.23% completed.\n",
      "\n",
      " Trained 2700 examples. 13.74% completed.\n",
      "\n",
      " Trained 2800 examples. 14.25% completed.\n",
      "\n",
      " Trained 2900 examples. 14.75% completed.\n",
      "\n",
      " Trained 3000 examples. 15.26% completed.\n",
      "\n",
      " Trained 3100 examples. 15.77% completed.\n",
      "\n",
      " Trained 3200 examples. 16.28% completed.\n",
      "\n",
      " Trained 3300 examples. 16.79% completed.\n",
      "\n",
      " Trained 3400 examples. 17.3% completed.\n",
      "\n",
      " Trained 3500 examples. 17.81% completed.\n",
      "\n",
      " Trained 3600 examples. 18.32% completed.\n",
      "\n",
      " Trained 3700 examples. 18.82% completed.\n",
      "\n",
      " Trained 3800 examples. 19.33% completed.\n",
      "\n",
      " Trained 3900 examples. 19.84% completed.\n",
      "\n",
      " Trained 4000 examples. 20.35% completed.\n",
      "\n",
      " Trained 4100 examples. 20.86% completed.\n",
      "\n",
      " Trained 4200 examples. 21.37% completed.\n",
      "\n",
      " Trained 4300 examples. 21.88% completed.\n",
      "\n",
      " Trained 4400 examples. 22.39% completed.\n",
      "\n",
      " Trained 4500 examples. 22.89% completed.\n",
      "\n",
      " Trained 4600 examples. 23.4% completed.\n",
      "\n",
      " Trained 4700 examples. 23.91% completed.\n",
      "\n",
      " Trained 4800 examples. 24.42% completed.\n",
      "\n",
      " Trained 4900 examples. 24.93% completed.\n",
      "\n",
      " Trained 5000 examples. 25.44% completed.\n",
      "\n",
      " Trained 5100 examples. 25.95% completed.\n",
      "\n",
      " Trained 5200 examples. 26.46% completed.\n",
      "\n",
      " Trained 5300 examples. 26.97% completed.\n",
      "\n",
      " Trained 5400 examples. 27.47% completed.\n",
      "\n",
      " Trained 5500 examples. 27.98% completed.\n",
      "\n",
      " Trained 5600 examples. 28.49% completed.\n",
      "\n",
      " Trained 5700 examples. 29.0% completed.\n",
      "\n",
      " Trained 5800 examples. 29.51% completed.\n",
      "\n",
      " Trained 5900 examples. 30.02% completed.\n",
      "\n",
      " Trained 6000 examples. 30.53% completed.\n",
      "\n",
      " Trained 6100 examples. 31.04% completed.\n",
      "\n",
      " Trained 6200 examples. 31.54% completed.\n",
      "\n",
      " Trained 6300 examples. 32.05% completed.\n",
      "\n",
      " Trained 6400 examples. 32.56% completed.\n",
      "\n",
      " Trained 6500 examples. 33.07% completed.\n",
      "\n",
      " Trained 6600 examples. 33.58% completed.\n",
      "\n",
      " Trained 6700 examples. 34.09% completed.\n",
      "\n",
      " Trained 6800 examples. 34.6% completed.\n",
      "\n",
      " Trained 6900 examples. 35.11% completed.\n",
      "\n",
      " Trained 7000 examples. 35.61% completed.\n",
      "\n",
      " Trained 7100 examples. 36.12% completed.\n",
      "\n",
      " Trained 7200 examples. 36.63% completed.\n",
      "\n",
      " Trained 7300 examples. 37.14% completed.\n",
      "\n",
      " Trained 7400 examples. 37.65% completed.\n",
      "\n",
      " Trained 7500 examples. 38.16% completed.\n",
      "\n",
      " Trained 7600 examples. 38.67% completed.\n",
      "\n",
      " Trained 7700 examples. 39.18% completed.\n",
      "\n",
      " Trained 7800 examples. 39.68% completed.\n",
      "\n",
      " Trained 7900 examples. 40.19% completed.\n",
      "\n",
      " Trained 8000 examples. 40.7% completed.\n",
      "\n",
      " Trained 8100 examples. 41.21% completed.\n",
      "\n",
      " Trained 8200 examples. 41.72% completed.\n",
      "\n",
      " Trained 8300 examples. 42.23% completed.\n",
      "\n",
      " Trained 8400 examples. 42.74% completed.\n",
      "\n",
      " Trained 8500 examples. 43.25% completed.\n",
      "\n",
      " Trained 8600 examples. 43.75% completed.\n",
      "\n",
      " Trained 8700 examples. 44.26% completed.\n",
      "\n",
      " Trained 8800 examples. 44.77% completed.\n",
      "\n",
      " Trained 8900 examples. 45.28% completed.\n",
      "\n",
      " Trained 9000 examples. 45.79% completed.\n",
      "\n",
      " Trained 9100 examples. 46.3% completed.\n",
      "\n",
      " Trained 9200 examples. 46.81% completed.\n",
      "\n",
      " Trained 9300 examples. 47.32% completed.\n",
      "\n",
      " Trained 9400 examples. 47.82% completed.\n",
      "\n",
      " Trained 9500 examples. 48.33% completed.\n",
      "\n",
      " Trained 9600 examples. 48.84% completed.\n",
      "\n",
      " Trained 9700 examples. 49.35% completed.\n",
      "\n",
      " Trained 9800 examples. 49.86% completed.\n",
      "\n",
      " Trained 9900 examples. 50.37% completed.\n",
      "\n",
      " Trained 10000 examples. 50.88% completed.\n",
      "\n",
      " Trained 10100 examples. 51.39% completed.\n",
      "\n",
      " Trained 10200 examples. 51.9% completed.\n",
      "\n",
      " Trained 10300 examples. 52.4% completed.\n",
      "\n",
      " Trained 10400 examples. 52.91% completed.\n",
      "\n",
      " Trained 10500 examples. 53.42% completed.\n",
      "\n",
      " Trained 10600 examples. 53.93% completed.\n",
      "\n",
      " Trained 10700 examples. 54.44% completed.\n",
      "\n",
      " Trained 10800 examples. 54.95% completed.\n",
      "\n",
      " Trained 10900 examples. 55.46% completed.\n",
      "\n",
      " Trained 11000 examples. 55.97% completed.\n",
      "\n",
      " Trained 11100 examples. 56.47% completed.\n",
      "\n",
      " Trained 11200 examples. 56.98% completed.\n",
      "\n",
      " Trained 11300 examples. 57.49% completed.\n",
      "\n",
      " Trained 11400 examples. 58.0% completed.\n",
      "\n",
      " Trained 11500 examples. 58.51% completed.\n",
      "\n",
      " Trained 11600 examples. 59.02% completed.\n",
      "\n",
      " Trained 11700 examples. 59.53% completed.\n",
      "\n",
      " Trained 11800 examples. 60.04% completed.\n",
      "\n",
      " Trained 11900 examples. 60.54% completed.\n",
      "\n",
      " Trained 12000 examples. 61.05% completed.\n",
      "\n",
      " Trained 12100 examples. 61.56% completed.\n",
      "\n",
      " Trained 12200 examples. 62.07% completed.\n",
      "\n",
      " Trained 12300 examples. 62.58% completed.\n",
      "\n",
      " Trained 12400 examples. 63.09% completed.\n",
      "\n",
      " Trained 12500 examples. 63.6% completed.\n",
      "\n",
      " Trained 12600 examples. 64.11% completed.\n",
      "\n",
      " Trained 12700 examples. 64.61% completed.\n",
      "\n",
      " Trained 12800 examples. 65.12% completed.\n",
      "\n",
      " Trained 12900 examples. 65.63% completed.\n",
      "\n",
      " Trained 13000 examples. 66.14% completed.\n",
      "\n",
      " Trained 13100 examples. 66.65% completed.\n",
      "\n",
      " Trained 13200 examples. 67.16% completed.\n",
      "\n",
      " Trained 13300 examples. 67.67% completed.\n",
      "\n",
      " Trained 13400 examples. 68.18% completed.\n",
      "\n",
      " Trained 13500 examples. 68.68% completed.\n",
      "\n",
      " Trained 13600 examples. 69.19% completed.\n",
      "\n",
      " Trained 13700 examples. 69.7% completed.\n",
      "\n",
      " Trained 13800 examples. 70.21% completed.\n",
      "\n",
      " Trained 13900 examples. 70.72% completed.\n",
      "\n",
      " Trained 14000 examples. 71.23% completed.\n",
      "\n",
      " Trained 14100 examples. 71.74% completed.\n",
      "\n",
      " Trained 14200 examples. 72.25% completed.\n",
      "\n",
      " Trained 14300 examples. 72.76% completed.\n",
      "\n",
      " Trained 14400 examples. 73.26% completed.\n",
      "\n",
      " Trained 14500 examples. 73.77% completed.\n",
      "\n",
      " Trained 14600 examples. 74.28% completed.\n",
      "\n",
      " Trained 14700 examples. 74.79% completed.\n",
      "\n",
      " Trained 14800 examples. 75.3% completed.\n",
      "\n",
      " Trained 14900 examples. 75.81% completed.\n",
      "\n",
      " Trained 15000 examples. 76.32% completed.\n",
      "\n",
      " Trained 15100 examples. 76.83% completed.\n",
      "\n",
      " Trained 15200 examples. 77.33% completed.\n",
      "\n",
      " Trained 15300 examples. 77.84% completed.\n",
      "\n",
      " Trained 15400 examples. 78.35% completed.\n",
      "\n",
      " Trained 15500 examples. 78.86% completed.\n",
      "\n",
      " Trained 15600 examples. 79.37% completed.\n",
      "\n",
      " Trained 15700 examples. 79.88% completed.\n",
      "\n",
      " Trained 15800 examples. 80.39% completed.\n",
      "\n",
      " Trained 15900 examples. 80.9% completed.\n",
      "\n",
      " Trained 16000 examples. 81.4% completed.\n",
      "\n",
      " Trained 16100 examples. 81.91% completed.\n",
      "\n",
      " Trained 16200 examples. 82.42% completed.\n",
      "\n",
      " Trained 16300 examples. 82.93% completed.\n",
      "\n",
      " Trained 16400 examples. 83.44% completed.\n",
      "\n",
      " Trained 16500 examples. 83.95% completed.\n",
      "\n",
      " Trained 16600 examples. 84.46% completed.\n",
      "\n",
      " Trained 16700 examples. 84.97% completed.\n",
      "\n",
      " Trained 16800 examples. 85.47% completed.\n",
      "\n",
      " Trained 16900 examples. 85.98% completed.\n",
      "\n",
      " Trained 17000 examples. 86.49% completed.\n",
      "\n",
      " Trained 17100 examples. 87.0% completed.\n",
      "\n",
      " Trained 17200 examples. 87.51% completed.\n",
      "\n",
      " Trained 17300 examples. 88.02% completed.\n",
      "\n",
      " Trained 17400 examples. 88.53% completed.\n",
      "\n",
      " Trained 17500 examples. 89.04% completed.\n",
      "\n",
      " Trained 17600 examples. 89.54% completed.\n",
      "\n",
      " Trained 17700 examples. 90.05% completed.\n",
      "\n",
      " Trained 17800 examples. 90.56% completed.\n",
      "\n",
      " Trained 17900 examples. 91.07% completed.\n",
      "\n",
      " Trained 18000 examples. 91.58% completed.\n",
      "\n",
      " Trained 18100 examples. 92.09% completed.\n",
      "\n",
      " Trained 18200 examples. 92.6% completed.\n",
      "\n",
      " Trained 18300 examples. 93.11% completed.\n",
      "\n",
      " Trained 18400 examples. 93.61% completed.\n",
      "\n",
      " Trained 18500 examples. 94.12% completed.\n",
      "\n",
      " Trained 18600 examples. 94.63% completed.\n",
      "\n",
      " Trained 18700 examples. 95.14% completed.\n",
      "\n",
      " Trained 18800 examples. 95.65% completed.\n",
      "\n",
      " Trained 18900 examples. 96.16% completed.\n",
      "\n",
      " Trained 19000 examples. 96.67% completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Trained 19100 examples. 97.18% completed.\n",
      "\n",
      " Trained 19200 examples. 97.69% completed.\n",
      "\n",
      " Trained 19300 examples. 98.19% completed.\n",
      "\n",
      " Trained 19400 examples. 98.7% completed.\n",
      "\n",
      " Trained 19500 examples. 99.21% completed.\n",
      "\n",
      " Trained 19600 examples. 99.72% completed.\n",
      "\n",
      " Trained 19700 examples. 100.23% completed.\n",
      "\n",
      " 19600\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "for i in range(0, X_train.shape[0], batch_size):\n",
    "    start = i\n",
    "    end = i + batch_size\n",
    "    pipeline.fit(X_train[start : end], Y_train[start : end])\n",
    "    print('\\n Trained {} examples. {}% completed.'.format(end, round(end/X_train.shape[0] * 100, 2)))\n",
    "    end = start\n",
    "    \n",
    "    if end == (X_train.shape[0] // 100) * 100:\n",
    "        print('\\n Trained {} examples. {}% completed.'.format(end, round(end/X_train.shape[0] * 100, 2)))\n",
    "        pipeline.fit(X_train[end:], Y_train[end:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test model\n",
    "Report the precision, recall, f1 score for each output category of the dataset, and overall accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_classification(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for idx, col in enumerate(y_test):\n",
    "        set_y_pair = (y_test[col], y_pred[:, idx])\n",
    "        avg='weighted'\n",
    "        rep_col = \"{}\\n\\tPrecision: {:.2f}%\\n\\tRecall: {:.2f}%\\n\\tF1 Score: {:.2f}%\\n\".format(col,\n",
    "                                                                                 precision_score(*set_y_pair, average=avg), \n",
    "                                                                                 recall_score(*set_y_pair, average=avg), \n",
    "                                                                                 f1_score(*set_y_pair, average=avg))\n",
    "        print(rep_col)\n",
    "        \n",
    "    print('Accuracy Score: {:.2f}%'.format(np.mean(y_test.values == y_pred)))\n",
    "\n",
    "    return np.mean(y_test.values == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "\tPrecision: 0.64%\n",
      "\tRecall: 0.76%\n",
      "\tF1 Score: 0.67%\n",
      "\n",
      "request\n",
      "\tPrecision: 0.72%\n",
      "\tRecall: 0.76%\n",
      "\tF1 Score: 0.74%\n",
      "\n",
      "offer\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "aid_related\n",
      "\tPrecision: 0.50%\n",
      "\tRecall: 0.55%\n",
      "\tF1 Score: 0.50%\n",
      "\n",
      "medical_help\n",
      "\tPrecision: 0.86%\n",
      "\tRecall: 0.88%\n",
      "\tF1 Score: 0.87%\n",
      "\n",
      "medical_products\n",
      "\tPrecision: 0.91%\n",
      "\tRecall: 0.90%\n",
      "\tF1 Score: 0.91%\n",
      "\n",
      "search_and_rescue\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.96%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faustina/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "security\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.96%\n",
      "\n",
      "military\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "child_alone\n",
      "\tPrecision: 1.00%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 1.00%\n",
      "\n",
      "water\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.91%\n",
      "\tF1 Score: 0.89%\n",
      "\n",
      "food\n",
      "\tPrecision: 0.80%\n",
      "\tRecall: 0.88%\n",
      "\tF1 Score: 0.83%\n",
      "\n",
      "shelter\n",
      "\tPrecision: 0.85%\n",
      "\tRecall: 0.88%\n",
      "\tF1 Score: 0.86%\n",
      "\n",
      "clothing\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "money\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "missing_people\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "refugees\n",
      "\tPrecision: 0.94%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "death\n",
      "\tPrecision: 0.90%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "other_aid\n",
      "\tPrecision: 0.78%\n",
      "\tRecall: 0.86%\n",
      "\tF1 Score: 0.81%\n",
      "\n",
      "infrastructure_related\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.90%\n",
      "\tF1 Score: 0.89%\n",
      "\n",
      "transport\n",
      "\tPrecision: 0.91%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "buildings\n",
      "\tPrecision: 0.91%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "electricity\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "tools\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "hospitals\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "shops\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "aid_centers\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "other_infrastructure\n",
      "\tPrecision: 0.91%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "weather_related\n",
      "\tPrecision: 0.61%\n",
      "\tRecall: 0.71%\n",
      "\tF1 Score: 0.61%\n",
      "\n",
      "floods\n",
      "\tPrecision: 0.85%\n",
      "\tRecall: 0.91%\n",
      "\tF1 Score: 0.88%\n",
      "\n",
      "storm\n",
      "\tPrecision: 0.83%\n",
      "\tRecall: 0.90%\n",
      "\tF1 Score: 0.86%\n",
      "\n",
      "fire\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "earthquake\n",
      "\tPrecision: 0.83%\n",
      "\tRecall: 0.90%\n",
      "\tF1 Score: 0.85%\n",
      "\n",
      "cold\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "other_weather\n",
      "\tPrecision: 0.91%\n",
      "\tRecall: 0.44%\n",
      "\tF1 Score: 0.57%\n",
      "\n",
      "direct_report\n",
      "\tPrecision: 0.70%\n",
      "\tRecall: 0.76%\n",
      "\tF1 Score: 0.72%\n",
      "\n",
      "Accuracy Score: 0.90%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8985000339167006"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = pipeline.predict(X_test)\n",
    "\n",
    "report_classification(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x7fbabc8fd320>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          max_samples=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=1,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=True),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x7fbabc8fd320>,\n",
       "                 vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        max_samples=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=1,\n",
       "                                                        n_jobs=None,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=None,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=True),\n",
       "                       n_jobs=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 1,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': True,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=1,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=True),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters)\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test model\n",
    "Show the precision, recall and overall accuracy of the tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "report_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Improve model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile web_app/models/train_classifier.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
