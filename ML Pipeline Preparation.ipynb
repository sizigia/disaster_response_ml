{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "### 1. Import libraries and load data from database\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "nltk.download(['averaged_perceptron_tagger', 'wordnet'])\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from helpers import report_classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///web_app/data/DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterResponseData', engine)\n",
    "X = df['message']\n",
    "Y = df[df.columns[-36:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Transforms a text to clean tokens, where every token is a word converted to lower case,\n",
    "    passed to a part-of-speech tagger and lemmatized accordingly.\n",
    "    Words recognized as stopwords are ommitted.\n",
    "    \n",
    "    Input:\n",
    "        text (str)\n",
    "        \n",
    "    Output:\n",
    "        clean_tokens (list): list of clean tokens (words converted to lower case and lemmatized)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    clean_tokens = []\n",
    "    \n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag[0] in ['A', 'R', 'N', 'V']:\n",
    "            tag = tag[0].lower()\n",
    "            clean_token = lemmatizer.lemmatize(word, pos=tag)\n",
    "        else:\n",
    "            clean_token = word\n",
    "            \n",
    "        if clean_token not in stopwords.words('english'):\n",
    "            clean_tokens.append(clean_token)\n",
    "        \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline takes in the `message` column as input and outputs classification results on the other 36 categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(warm_start=True))),\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 4.3min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.1s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=22.0min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=True),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test model\n",
    "Report the precision, recall, f1 score for each output category of the dataset, and overall accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "\tPrecision: 0.81%\n",
      "\tRecall: 0.82%\n",
      "\tF1 Score: 0.81%\n",
      "\n",
      "request\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.89%\n",
      "\tF1 Score: 0.88%\n",
      "\n",
      "offer\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "aid_related\n",
      "\tPrecision: 0.79%\n",
      "\tRecall: 0.79%\n",
      "\tF1 Score: 0.79%\n",
      "\n",
      "medical_help\n",
      "\tPrecision: 0.90%\n",
      "\tRecall: 0.92%\n",
      "\tF1 Score: 0.89%\n",
      "\n",
      "medical_products\n",
      "\tPrecision: 0.94%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "search_and_rescue\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.96%\n",
      "\n",
      "security\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "military\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "child_alone\n",
      "\tPrecision: 1.00%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 1.00%\n",
      "\n",
      "water\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "food\n",
      "\tPrecision: 0.94%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "shelter\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.93%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "clothing\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "money\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "missing_people\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "refugees\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.96%\n",
      "\n",
      "death\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "other_aid\n",
      "\tPrecision: 0.82%\n",
      "\tRecall: 0.86%\n",
      "\tF1 Score: 0.81%\n",
      "\n",
      "infrastructure_related\n",
      "\tPrecision: 0.89%\n",
      "\tRecall: 0.93%\n",
      "\tF1 Score: 0.90%\n",
      "\n",
      "transport\n",
      "\tPrecision: 0.94%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "buildings\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "electricity\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "tools\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "hospitals\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "shops\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "aid_centers\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "other_infrastructure\n",
      "\tPrecision: 0.92%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "weather_related\n",
      "\tPrecision: 0.87%\n",
      "\tRecall: 0.88%\n",
      "\tF1 Score: 0.87%\n",
      "\n",
      "floods\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "storm\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.93%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "fire\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "earthquake\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "cold\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "other_weather\n",
      "\tPrecision: 0.92%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "direct_report\n",
      "\tPrecision: 0.85%\n",
      "\tRecall: 0.86%\n",
      "\tF1 Score: 0.84%\n",
      "\n",
      "Accuracy Score: 0.95%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9477036924015868"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = pipeline.predict(X_test)\n",
    "\n",
    "report_classification(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x7ffab08ce950>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          max_samples=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=100,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=True),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': True,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x7ffab08ce950>,\n",
       "                 vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        max_samples=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_jobs=None,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=None,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=True),\n",
       "                       n_jobs=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': True,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=True),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                                   max_features='auto',\n",
       "                                                                   max_leaf_nodes=None,\n",
       "                                                                   max_samples=None,\n",
       "                                                                   min_impurity_decrease=0.0,\n",
       "                                                                   min_impurity_split=None,\n",
       "                                                                   min_samples_leaf=1,\n",
       "                                                                   min_samples_split=2,\n",
       "                                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                                   n_estimators=100,\n",
       "                                                                   n_jobs=None,\n",
       "                                                                   oob_score=False,\n",
       "                                                                   random_state=None,\n",
       "                                                                   verbose=0,\n",
       "                                                                   warm_start=True))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'clf__estimator': (MultinomialNB(), \n",
    "                       RandomForestClassifier(warm_start=True))\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters, cv=3, n_jobs=1, verbose=10)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 2.8min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.6s\n",
      "[CV]  clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), score=0.162, total= 4.3min\n",
      "[CV] clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 2.8min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.6s\n",
      "[CV]  clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), score=0.170, total= 4.2min\n",
      "[CV] clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  8.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 2.8min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.6s\n",
      "[CV]  clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), score=0.161, total= 4.2min\n",
      "[CV] clf__estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=True) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 12.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 2.8min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=10.8min\n",
      "[CV]  clf__estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=True), score=0.260, total=15.6min\n",
      "[CV] clf__estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=True) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 28.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 2.8min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.1s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=10.9min\n",
      "[CV]  clf__estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=True), score=0.268, total=15.7min\n",
      "[CV] clf__estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=True) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 44.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 2.8min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=10.8min\n",
      "[CV]  clf__estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=True), score=0.261, total=15.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 59.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 59.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 4.2min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.1s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=22.4min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                                   max_features='auto',\n",
       "                                                                   max_leaf_nodes=None,\n",
       "                                                                   max_samples=None,\n",
       "                                                                   min_impurity_decrease=0.0,\n",
       "                                                                   min_impurity_split=None,\n",
       "                                                                   min_samples_leaf=1,\n",
       "                                                                   min_samples_split=2,\n",
       "                                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                                   n_estimators=100,\n",
       "                                                                   n_jobs=None,\n",
       "                                                                   oob_score=False,\n",
       "                                                                   random_state=None,\n",
       "                                                                   verbose=0,\n",
       "                                                                   warm_start=True))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=True),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=True)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_2 = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total= 4.2min\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.1s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=22.7min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=True),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test model\n",
    "Show the precision, recall and overall accuracy of the tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "\tPrecision: 0.81%\n",
      "\tRecall: 0.83%\n",
      "\tF1 Score: 0.81%\n",
      "\n",
      "request\n",
      "\tPrecision: 0.89%\n",
      "\tRecall: 0.89%\n",
      "\tF1 Score: 0.88%\n",
      "\n",
      "offer\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid_related\n",
      "\tPrecision: 0.79%\n",
      "\tRecall: 0.79%\n",
      "\tF1 Score: 0.79%\n",
      "\n",
      "medical_help\n",
      "\tPrecision: 0.89%\n",
      "\tRecall: 0.92%\n",
      "\tF1 Score: 0.89%\n",
      "\n",
      "medical_products\n",
      "\tPrecision: 0.94%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "search_and_rescue\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.96%\n",
      "\n",
      "security\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "military\n",
      "\tPrecision: 0.96%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "child_alone\n",
      "\tPrecision: 1.00%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 1.00%\n",
      "\n",
      "water\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "food\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.94%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "shelter\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.93%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "clothing\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "money\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "missing_people\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "refugees\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "death\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "other_aid\n",
      "\tPrecision: 0.82%\n",
      "\tRecall: 0.86%\n",
      "\tF1 Score: 0.80%\n",
      "\n",
      "infrastructure_related\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.93%\n",
      "\tF1 Score: 0.90%\n",
      "\n",
      "transport\n",
      "\tPrecision: 0.94%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "buildings\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "electricity\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "tools\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "hospitals\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "shops\n",
      "\tPrecision: 0.99%\n",
      "\tRecall: 1.00%\n",
      "\tF1 Score: 0.99%\n",
      "\n",
      "aid_centers\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "other_infrastructure\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.96%\n",
      "\tF1 Score: 0.94%\n",
      "\n",
      "weather_related\n",
      "\tPrecision: 0.88%\n",
      "\tRecall: 0.88%\n",
      "\tF1 Score: 0.87%\n",
      "\n",
      "floods\n",
      "\tPrecision: 0.95%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.95%\n",
      "\n",
      "storm\n",
      "\tPrecision: 0.93%\n",
      "\tRecall: 0.93%\n",
      "\tF1 Score: 0.93%\n",
      "\n",
      "fire\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.99%\n",
      "\tF1 Score: 0.98%\n",
      "\n",
      "earthquake\n",
      "\tPrecision: 0.97%\n",
      "\tRecall: 0.97%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "cold\n",
      "\tPrecision: 0.98%\n",
      "\tRecall: 0.98%\n",
      "\tF1 Score: 0.97%\n",
      "\n",
      "other_weather\n",
      "\tPrecision: 0.92%\n",
      "\tRecall: 0.95%\n",
      "\tF1 Score: 0.92%\n",
      "\n",
      "direct_report\n",
      "\tPrecision: 0.85%\n",
      "\tRecall: 0.86%\n",
      "\tF1 Score: 0.84%\n",
      "\n",
      "Accuracy Score: 0.95%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9477418370460787"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_2 = pipeline_2.predict(X_test)\n",
    "\n",
    "report_classification(Y_test, Y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Export model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('web_app/models/classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline_2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Use this notebook to complete `train_classifier.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting web_app/models/train_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile web_app/models/train_classifier.py\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "nltk.download(['averaged_perceptron_tagger', 'wordnet', 'stopwords'])\n",
    "\n",
    "\n",
    "def load_data(database_filepath):\n",
    "    \"\"\"\n",
    "    Creates an Engine instance with the path provided, and reads the SQL table that stores the cleaned data.\n",
    "    Returns features, labels and category names for the dataset.\n",
    "    \"\"\"\n",
    "    engine = create_engine('sqlite:///' + database_filepath)\n",
    "    \n",
    "    df = pd.read_sql_table('DisasterResponseData', engine)\n",
    "    \n",
    "    X = df['message']\n",
    "    y = df[df.columns[-36:]]\n",
    "    \n",
    "    return X, y, df.columns[-36:]\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Transforms a text to clean tokens, where every token is a word converted to lower case,\n",
    "    passed to a part-of-speech tagger and lemmatized accordingly.\n",
    "    Words recognized as stopwords are ommitted.\n",
    "    \n",
    "    Input:\n",
    "        text (str)\n",
    "        \n",
    "    Output:\n",
    "        clean_tokens (list): list of clean tokens (words converted to lower case and lemmatized)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    clean_tokens = []\n",
    "    \n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag[0] in ['A', 'R', 'N', 'V']:\n",
    "            tag = tag[0].lower()\n",
    "            clean_token = lemmatizer.lemmatize(word, pos=tag)\n",
    "        else:\n",
    "            clean_token = word\n",
    "            \n",
    "        if clean_token not in stopwords.words('english'):\n",
    "            clean_tokens.append(clean_token)\n",
    "        \n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    No input needed. Returns a pipeline with the next steps:\n",
    "        1. vect - Converts a collection of text documents to a matrix of token counts\n",
    "        2. tfidf - Transforms a count matrix to a normalized *term-frequency* or \n",
    "                  *term-frequency times inverse document-frequency* representation\n",
    "        3. clf - Multi target random forest classification. It reuses the solution \n",
    "                    of the previous call to fit and add more estimators to the ensemble.    \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(warm_start=True))),\n",
    "    ], verbose=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    \"\"\"\n",
    "    Takes the model, X and Y test set and category names, and returns precision, recall and F1 score \n",
    "    for every feature in the dataset, and the overall accuracy of the model.\n",
    "    \n",
    "    Input:\n",
    "        model ():\n",
    "        X_test (pandas.core.series.Series): a subset of Y with the purpose of testing the model\n",
    "        Y_test (pandas.core.series.Series): predictions made with X_test by the model\n",
    "        category_names (): \n",
    "        \n",
    "    Output:\n",
    "        Prints out the following format\n",
    "            feature_name\n",
    "                Precision: __%\n",
    "                Recall: __%\n",
    "                F1 Score: __%\n",
    "                \n",
    "                ...\n",
    "                \n",
    "                Accuracy Score: __%\n",
    "                \n",
    "        And also returns the full value of accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    for idx, col in enumerate(category_names):\n",
    "        set_Y_pair = (Y_test[col], Y_pred[:, idx])\n",
    "        avg='weighted'\n",
    "        rep_col = \"{}\\n\\tPrecision: {:.2f}%\\n\\tRecall: {:.2f}%\\n\\tF1 Score: {:.2f}%\\n\".format(col,\n",
    "                                                                                 precision_score(*set_Y_pair, average=avg), \n",
    "                                                                                 recall_score(*set_Y_pair, average=avg), \n",
    "                                                                                 f1_score(*set_Y_pair, average=avg))\n",
    "        print(rep_col)\n",
    "        \n",
    "    print('Accuracy Score: {:.2f}%'.format(np.mean(Y_pred.values == Y_pred)))\n",
    "\n",
    "    return np.mean(Y_pred.values == Y_pred)\n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    \"\"\"\n",
    "    Takes in the trained model and a path where to store it.\n",
    "    Dumps the model in a pickle to reuse it later.\n",
    "    \"\"\"\n",
    "    with open(model_filepath, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) == 3:\n",
    "        database_filepath, model_filepath = sys.argv[1:]\n",
    "        print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        X, Y, category_names = load_data(database_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        \n",
    "        print('Building model...')\n",
    "        model = build_model()\n",
    "        \n",
    "        print('Training model...')\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        print('Evaluating model...')\n",
    "        evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "        print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('Trained model saved!')\n",
    "\n",
    "    else:\n",
    "        print('Please provide the filepath of the disaster messages database '\\\n",
    "              'as the first argument and the filepath of the pickle file to '\\\n",
    "              'save the model to as the second argument. \\n\\nExample: python '\\\n",
    "              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
